import seqdata as sq
import sys
import pysam
import re as re
import pybedtools as pbt
import os
import os.path


"""
Process a peak file and read file to generate all subsequent files required to perform
ChIP-seq peak clustering using a Dirichlet model.
"""


#Globals - mostly file names that are required in multiple methods
peak_file = None
window_size = 0
central_result = None
central_fasta = None
central_f_fasta = None
fasta_file = None
read_file = None
segment_size = 0
seg_file = None
k_len = 0
kmer_count = 0
kmer_file = None
for_clustering = None
clusters = 0


## Begin with peak file
## Find central window around summit

"""
Usage: narrowpeak file, output name, window size
A method which takes a narrow peak file and restricts each peak to a window around the peak's summit
e.g. a 500bp window would have 250bp either side of the summit
"""
def summitPeaksNarrow(peakFile, window, ident):
    global central_result
    global peak_file
    global window_size
    global identifier
    identifier = ident
    peak_file = peakFile
    window_size = window
    if peakFile.endswith(".narrowPeak"):
        centralResult = re.sub(".narrowPeak", "_central_"+str(window)+"_"+identifier+".out", peakFile)
    elif peakFile.endswith(".bed"):
        centralResult = re.sub(".bed", "_central_"+str(window)+"_"+identifier+".out", peakFile)
    else:
        print "Incorrect file provided - not .bed or .narrowpeak"
        sys.exit()
    central_result = centralResult
    if os.path.isfile(central_result):
        print "Using existing file: "+central_result
        return
    o = open(centralResult, 'w')
    bed = sq.BedFile(peakFile, 'Optional')
    w = int(window)
    hw = abs(w/2)
    for line in bed:
        c,s,e = line.loc()
        try:
            summit = int(s) + line.summit
        except AttributeError:
            summit = int(s) + 0
        ns = summit - hw
        ne = summit + hw
        o.write("%s\t%s\t%s\t%s\t%s\n"%(c, str(ns), str(ne), line.name, line.score))
    o.close()

## Generate fasta versions of peak file
""" centralResult - bed file """
def getFa(centralResult, fastaFile):
    global central_fasta
    global fasta_file
    fasta_file = fastaFile
    fastaResult = re.sub("out", "fa", centralResult)
    central_fasta = fastaResult
    if os.path.isfile(central_fasta):
        print "Using existing file: "+central_fasta
        return
    bed = centralResult
    b = pbt.BedTool(bed)
    bSeq = b.sequence(fastaFile)
    bSeq.save_seqs(fastaResult)

##Get formatted version of fasta file
def getFaFormat(fastaFile):
    global central_f_fasta
    fasta = open(central_fasta)
    fastaForm = re.sub("fa", "f.fa", fastaFile)
    central_f_fasta = fastaForm
    if os.path.isfile(central_f_fasta):
        print "Using existing file: "+central_f_fasta
        return
    out = open(fastaForm, 'w')
    f = fasta.readlines()
    for i in range(0, len(f), 2):
        name = f[i].strip()
        seq = f[i+1].strip()
        out.write(name)
        out.write('\t')
        out.write(seq)
        out.write('\n')
    out.close()

##Once you have a uniform peak file, begin gathering counts
"""
Usage: central peak file, output name, bam file that peaks were generated from
A method which takes a central peak file and counts the pileup of reads
in each segment across the window
Returns a count vector suitable for dirichlet node
"""
def segmentCounts(centralResult, readfile, segment):
    global segment_size
    global read_file
    global seg_file
    segment_size = segment
    read_file = readfile
    segmentOut = re.sub("_central_", "_seg"+str(segment)+"_", centralResult)
    seg_file = segmentOut
    if os.path.isfile(seg_file):
        print "Using existing file: "+seg_file
        return
    segOut = open(segmentOut, 'w')
    bed = sq.BedFile(centralResult, 'Optional')
    reads = pysam.Samfile(readfile, "rb")
    seg = int(segment)
    ##How to control the spread of segment across window size?
    for peak in bed:
        p,s,e = peak.loc()
        s = int(s)
        e = int(e)
        win = e-s
        lenList = win/seg
        segList = []
        for i in range(s, e, seg):
            count = len(list(reads.fetch(p, i, i+seg)))
            segList.append(count+1)
        ##Ensure all peaks maintain same orientation
        ##If mode is beyond middle of list; reverse the list
        mode = segList.index(max(segList))
        if mode > ((lenList/2)-1):
            segList = segList[::-1]
        segList = map(str, segList)
        out = '\t'.join(segList)
        segOut.write(out)
        segOut.write('\n')
    segOut.close()

## To get kmer counts, first you need a list of kmers
"""
Usage: formatted fasta file (>name\tsequence) based on central peak windows,
output name, list of kmers to count
Formatted fasta file generated by getFa (.f.fa)
A method which takes fasta entries and counts the occurences of kmers within each entry
"""
def kmerCounts(centralFasta, kmerfile):
    global k_len
    global kmer_count
    global kmer_file
    #filename = formatted fasta file based on raw peaks
    bed = open(centralFasta)
    kmers = open(kmerfile)
    kmers = kmers.readlines()
    k = len(kmers[0].strip())
    k_len = k
    kmer_count = len(kmers)
    kmerOut = re.sub("_central_", "_"+str(k)+"mer_", central_result)
    kmer_file = kmerOut
    if os.path.isfile(kmer_file):
        print "Using existing file: "+kmer_file
        return
    o = open(kmerOut, 'w')
    for peak in bed:
        peak = peak.strip().split()
        segList = []
        for k in kmers:
            k = k.strip()
            reg = re.compile(k, re.IGNORECASE) #is ignoring case appropriate?
            matches = reg.findall(peak[1])#NON-OVERLAPPING - any alternatives?
            count = len(matches)
            segList.append(str(count+1))
##        print segList
        out = '\t'.join(segList)
        o.write(out)
        o.write('\n')
    o.close()

##Merge kmer and segment files - order must be alphabetical (by name of future node)
def mergeCounts(kmerOut, segmentOut):
    global for_clustering
    k = open(kmerOut)
    s = open(segmentOut)
    mergeOut = re.sub("_central_", "_"+str(k_len)+"mer_seg"+str(segment_size)+"_", central_result)
    for_clustering = mergeOut
    if os.path.isfile(for_clustering):
        print "Using existing file: "+for_clustering
        return
    m = open(mergeOut, 'w')
    kmer = k.readlines()
    seg = s.readlines()
    if len(seg) != len(kmer):
        print "Files must be same length"
        sys.exit()
    for i in range(0, len(kmer)):
        kl = kmer[i].strip().split('\t')
        sl = seg[i].strip().split('\t')
        new = []
        for j in range(0, len(kl)):
            new.append(kl[j])
        for k in range(0, len(sl)):
            new.append(sl[k])
        w = '\t'.join(new)
        m.write(w)
        m.write('\n')
    m.close()

#Clustering via EM occurs (via call to jar file from main)

"""Label each cluster generated by EM then get fasta files based
on bed files
"""
def getLabels():
    global clus_file_base
    clusFiles = []
    for i in range(0, clusters, 1):
        name = "%s%s%i%s"%(for_clustering, "_bin_", i, ".out")
        clusFiles.append(name)
    y=0
    for f in clusFiles:
        getPeaks(f, central_result, y)
        bedName = re.sub(".out_bin_..out", "_c"+str(clusters)+"_b"+str(y)+".out", f)
        getFa(bedName, fasta_file)
        y+=1
    clus_file_base = re.sub(".out_bin_..out", "_c"+str(clusters), clusFiles[0])

"""Given a cluster result file, create a bed file representing the original peaks
which are part of that cluster
"""
def getPeaks(clusterFile, peakFile, out_file, y):
    out = open(out_file, 'w')
    clusNum = open(clusterFile)
    segFile = open(peakFile)
    peaks = segFile.readlines()
    for line in clusNum:
        line = line.strip().split('\t')
        index = int(line[0])
        try:
            peak = peaks[index].strip().split('\t') ## Out by one error if header is left in! (subtract 1)
        except IndexError:
            print index
        new = [peak[0], peak[1], peak[2], peak[3], peak[4], str(y)]##retrieve different columns depending on input file
##        new = [peak[0], peak[1], peak[2], peak[5], peak[6], str(y)]
        peak = peaks[index].strip().split('\t') ## Out by one error if header is left in! (subtract 1)
        n = '\t'.join(new)
        out.write(n)
        out.write('\n')
    out.close()    
         
if __name__ == "__main__":

    """
    Process a peak file and read file to generate all subsequent files required to perform
    ChIP-seq peak clustering using a Dirichlet model.

    The user can choose to just generate the files or have this script run the clustering
    code as well. Currently, it is assumed that the clustering code will be run across
    a series of cluster sizes to identify the optimal size. This is reported in the output
    files generated by the jar file.
    """
    print len(sys.argv)
    if len(sys.argv) <= 1 or len(sys.argv)<7 or (len(sys.argv)>7 and len(sys.argv)<10):
        print "Usage: peakFile, windowSize, fastaRef, readFile, segmentSize, id"
        print "Usage: peakFile, windowSize, fastaRef, readFile, segmentSize, id, jarFile, minClus, maxClus"
        print "Usage: peakFile, windowSize, fastaRef, readFile, segmentSize, id, jarFile, minClus, maxClus, save?"
        exit()
    if len(sys.argv) == 7 or len(sys.argv) == 10 or len(sys.argv) == 11:
        ident = sys.argv[6] #test identifier

        print "summitPeaksNarrow"
        summitPeaksNarrow(sys.argv[1], sys.argv[2], ident) ##Requires handling for non narrowpeak ChIP peak files
    ##  fasta file
        print "getFa"
        getFa(central_result, sys.argv[3])
        print "getFaFormat"
        getFaFormat(central_fasta)
        #readfile, segment size
        print "segmentCounts"
        segmentCounts(central_result, sys.argv[4], sys.argv[5])
        if len(sys.argv) == 7:
            exit()

    if len(sys.argv) == 10:
        #Run MDD
        # jar file
        print "Launch Java"
        sys.stdout.flush()
        call = "%s %s %s %d %d"%("java -jar", sys.argv[7], seg_file, int(sys.argv[8]), int(sys.argv[9]) ) 
        os.system(call)

    if len(sys.argv) == 11:
        #Run MDD
        # jar file
        print "Launch Java"
        sys.stdout.flush()
        call = "%s %s %s %d %d %s"%("java -jar", sys.argv[7], seg_file, int(sys.argv[8]), int(sys.argv[9]), sys.argv[10] ) 
        os.system(call)

    """
    Identify the optimal cluster size and record peaks belonging
    to each cluster
    """

    compFile = "%s%s"%(seg_file, "_comp.out")
    cf = open(compFile)
    cf = cf.readlines()
    minComp = float(cf[0].strip().split("\t")[1])
    minBin = cf[0].strip().split("\t")[0]
    for line in cf:
        line = line.strip().split("\t")
        if float(line[1]) <  minComp:
            minComp = float(line[1])
            minBin = line[0]
        
    clusters = int(minBin)
    clusFiles = []
    for i in range(0, clusters, 1):
        name = "%s%s%i%s%i%s"%(seg_file, "_bin_", i, "_",clusters, ".out")
        clusFiles.append(name)
    y=0
    for f in clusFiles:
        getPeaks(f,central_result, seg_file+"_bin_c"+str(clusters)+"_"+str(y)+".out", y)
        y+=1

    if "/" in seg_file:
        folder = seg_file.rsplit("/", 0)
    else:
        folder = ""

    print "%s%d"%("cluster = ", clusters)
    print "%s%s%s"%("folder = '", folder, "'")
    print "%s%s%s"%("name = '", ident, "'")
    print "%s%s%s"%("fileBase = '", seg_file, "'")
    print "%s%s%s"%("narrow = '", peak_file, "'")









